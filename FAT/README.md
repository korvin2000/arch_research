# FAT (IEEE TGRS 2025)
### Mitigating Texture Bias: A Remote Sensing Super-Resolution Method Focusing on High-Frequency Texture Reconstruction
### æŠ‘åˆ¶çº¹ç†åè§ï¼š ä¸€ç§èšç„¦äºé«˜é¢‘çº¹ç†é‡å»ºçš„é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•

This repository is an official implementation of the paper "*[Mitigating Texture Bias: A Remote Sensing Super-Resolution Method Focusing on High-Frequency Texture Reconstruction](https://ieeexplore.ieee.org/document/10912673)*"

---


### :tada::tada: News :tada::tada:
- **2025/5/20**  âœ…**Code fully released**   
  *(Just finished undergrad thesis and new paper on 5/18. Who's coding frantically on 520 without a girlfriend? Oh it's me, the clown ğŸ¤¡)* 
- **2025/4/8**  ğŸš€ **Model code released**
- **2025/3/2** **RSSR25ğŸ¤— now on [HuggingFace](https://huggingface.co/datasets/fengyanzi/RSSR25/tree/main)**  
-  **2025/2/27**  **Double celebration!**  *Paper accepted by [IEEE TGRS](https://ieeexplore.ieee.org/document/10912673) 
    ğŸ‰*Another paper accepted by [CVPR 2025](https://arxiv.org/abs/2504.09621) on the same day ğŸ†*
- **2025/1/2**  ğŸ” **Paper received major revision decision**  

### :tada::tada: æ–°é—» :tada::tada:
- **2025/5/20**  âœ… ä»£ç å®Œæ•´å‘å¸ƒ
  *(5/18åˆšå®Œæˆæœ¬ç§‘æ¯•è®¾ä¸æ–°è®ºæ–‡ï¼Œæ˜¯è°520æ²¡æœ‰å¥³æœ‹å‹å´åœ¨ç–¯ç‹‚æ¶¦è‰²ä»£ç å†™è¯´æ˜ï¼ŸåŸæ¥æ˜¯æˆ‘è¿™ä¸ªå°ä¸‘ğŸ¤¡)*  
- **2025/4/8**  ğŸš€ æ¨¡å‹ä»£ç å…¬å¼€
- **2025/3/2** ğŸŒ RSSR25æ•°æ®é›†ç™»é™†[HuggingFace](https://huggingface.co/datasets/fengyanzi/RSSR25/tree/main) ğŸ¤—
-  **2025/2/27** âœ¨ åŒå–œä¸´é—¨ï¼  è®ºæ–‡è¢«IEEE TGRSæ¥æ”¶ ğŸ‰
  ğŸ‰* åŒæ—¥å¦ä¸€ç¯‡è®ºæ–‡è¢«[CVPR 2025](https://arxiv.org/abs/2504.09621)æ¥æ”¶ ğŸ†
- **2025/1/2**  ğŸ” è®ºæ–‡è·å¾—å¤§ä¿®å†³å®š  


---

## Abstract
> Super-resolution (SR) is an ill-posed problem because one low-resolution image can correspond to multiple high-resolution images. High-frequency details are significantly lost in low-resolution images. Existing deep learning based SR models excel in reconstructing low-frequency and regular textures but often fail to achieve high-quality reconstruction of super-resolution high-frequency textures. These models exhibit bias toward different texture regions, leading to imbalanced reconstruction across various areas. To address this issue and reduce model bias toward diverse texture patterns, we propose a frequency-aware super-resolution method that improves the reconstruction of high-frequency textures by incorporating local data distributions. First, we introduce the Frequency-Aware Transformer (FAT), which enhances the capability of Transformer-based models to extract frequency-domain and global features from remote sensing images. Moreover, we design a local extremum and variance-based loss function, which guides the model to reconstruct more realistic texture details by focusing on local data distribution. Finally, we construct a high-quality remote sensing super-resolution dataset named RSSR25. We also discover that denoising algorithms can serve as an effective enhancement method for existing public datasets to improve model performance. Extensive experiments on multiple datasets demonstrate that the proposed FAT achieves superior perceptual quality while maintaining high distortion metrics scores compared to state-of-the-art algorithms. The source code and dataset will be publicly available at https://github.com/fengyanzi/FAT.

---
## Network  
![framework](./docx/main.png)

---
## ğŸ“¦ Installation
Installing the virtual environment is very ***simple***, relying only on PyTorch and some extremely basic libraries such as ***tqdm*** and ***timm***, making it difficult to conflict with your existing virtual environment.

To install the necessary dependencies for ***FAT***, please follow the steps below:


```
# Clone using the web URL
git clone https://github.com/fengyanzi/FAT.git

# Create conda env
conda create -n FAT python=3.10
conda activate FAT

# Install Pytorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# Install other packages needed
pip install -r requirements.txt
```

> Please ensure that you have the correct version of PyTorch installed that matches your deviceâ€™s CUDA version. You can check your CUDA version and find the corresponding PyTorch build using the [PyTorch installation guide](https://pytorch.org/get-started/locally/).

---
## ğŸ“š Dataset

We have developed a high-quality remote sensing image super-resolution dataset named ***RSSR25***.

Dataset Details
- **Training set**: 5,000 images
- **Test set**: 150 images
- **Spatial resolution**: 0.06m to 1.5m
- **Image size**: Majority are 720Ã—720 pixels, with a small portion being slightly larger

### Important Note
> Both compressed and uncompressed versions are available on Baidu Cloud. **The compressed files may be corrupted**, please download the uncompressed version!!!
> éœ€è¦æ³¨æ„çš„æ˜¯ BaiduCloudä¸­æˆ‘ä»¬æä¾›äº†å‹ç¼©ä¸éå‹ç¼©å½¢å¼ï¼Œ**å‹ç¼©å½¢å¼çš„æ–‡ä»¶ä¼¼ä¹å­˜åœ¨æŸå**ï¼Œè¯·ä¸‹è½½éå‹ç¼©å½¢å¼æ–‡ä»¶ï¼ï¼ï¼

You can obtain the `RSSR25` dataset from:
- [Baidu Cloud](https://pan.baidu.com/s/1Ywy6W6eVLsJ7nVVoKf6HaQ?pwd=4321) 
- ğŸ¤—[Hugging Face](https://huggingface.co/datasets/fengyanzi/RSSR25/tree/main)


![RSSR25 Dataset Samples](./docx/dataset.png)

---
## ğŸŒˆ Train & Test

#### Dataset Structure
Before training/testing, organize your `datasets` directory as follows (example structure provided):

```
datasets/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ GT/          # High-resolution ground truth
â”‚   â”‚   â”œâ”€â”€ 000.png
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ 099.png
â”‚   â””â”€â”€ LR/          # Low-resolution input
â”‚       â”œâ”€â”€ 000.png
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ 099.png
â””â”€â”€ test/            # Test images
    â”œâ”€â”€ 000.png
    â”œâ”€â”€ ...
    â””â”€â”€ 099.png
```

#### Training
Run:
```bash
python train.py
```

Configurable arguments:
| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--data_dir` | str | `./datasets/train` | Training data path |
| `--save_dir` | str | `./checkpoints/v1` | Model save directory |
| `--save_cycle` | int | `5` | Save checkpoint every N epochs |
| `--resume` | str | `None` | Checkpoint path to resume training |
| `--lr` | float | `0.0001` | Learning rate |
| `--batch_size` | int | `2` | Training batch size |
| `--no-cuda` | flag | `False` | Disable GPU acceleration |
| `--epochs` | int | `100` | Total training epochs |

#### Inference
Run:
```bash
python inference.py
```

Configurable arguments:
| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--test_img` | str | `./datasets/test/` | Test images directory |
| `--model_path` | str | `./checkpoints/best.pth` | Pretrained model path |
| `--save_dir` | str | `./result/version1/` | Output save directory |
| `--no-cuda` | flag | `False` | Disable GPU acceleration (Not Supported Yet) |
| `--fp16` | flag | `False` | FP16 inference (Not Supported Yet) |

---

## ğŸï¸ Visualization

![img](./docx/test.png)

---
## (Real-ESRGAN)Downsampling Tool

We additionally provide an image degradation tool for creating training datasets for real-world image super-resolution models. This tool is developed based on [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN)'s degradation simulation approach and has been adapted to run in 2025.

How to run:
```bash
python ./utils/realdownsample/realesrgan_degration.py
```

---
##  Local Attribution Map
Additionally, I provide a user-friendly [LAM](https://github.com/fengyanzi/Local-Attribution-Map-for-Super-Resolution) diagnostic tool, that can be run in 2025.

The Local Attribution Map (LAM) is an interpretability tool designed for super-resolution tasks. It identifies the pixels in a low-resolution input image that have the most significant impact on the networkâ€™s output. By analyzing the local regions of the super-resolution result, LAM highlights the areas contributing the most to the reconstruction, providing insights into the model's decision-making process.

---

## ğŸ“– Citation

If you find our code useful, please consider citing our paper:

```
@article{yan2025mitigating,
  title={Mitigating texture bias: A remote sensing super-resolution method focusing on high-frequency texture reconstruction},
  author={Yan, Xinyu and Chen, Jiuchen and Xu, Qizhi and Li, Wei},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2025},
  publisher={IEEE}
}
```

## ğŸ˜Š Acknowledgement

We would like to thank the my coauthor of [MRF-NET](https://github.com/CastleChen339/MRF-Net) for their inspiring work, which has been a valuable reference for our research.
